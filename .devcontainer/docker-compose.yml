version: "3.8"
services:
  python-env:
    build:
      context: .
      dockerfile: ./dockerfile
    volumes:
      - ../:/workspaces
    # Overrides default command so things don't shut down after the process ends.
    command: sleep infinity
    networks:
      llm-network:

  ollama-service:
    build:
      context: .
      dockerfile: ./qwen2.5-coder.32b.dockerfile
    ports:
      - 11434:11434
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
    command: sleep infinity
    networks:
      llm-network:
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: ["gpu"]
            count: all

networks:
  llm-network:
    # driver: bridge
